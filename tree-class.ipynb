{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import codecs\n",
    "import time\n",
    "import random\n",
    "import dynet as dy\n",
    "import numpy as np\n",
    "\n",
    "from tree import Tree\n",
    "\n",
    "def read_dataset(filename):\n",
    "    return [Tree.from_sexpr(line.strip()) for line in codecs.open(filename,\"r\")]\n",
    "\n",
    "def get_vocabs(trees):\n",
    "    label_vocab = Counter()\n",
    "    word_vocab  = Counter()\n",
    "    for tree in trees:\n",
    "        label_vocab.update([n.label for n in tree.nonterms()])\n",
    "        word_vocab.update([l.label for l in tree.leaves()])\n",
    "    labels = [x for x,c in label_vocab.iteritems() if c > 0]\n",
    "    words  = [\"_UNK_\"] + [x for x,c in word_vocab.iteritems() if c > 0]\n",
    "    l2i = {l:i for i,l in enumerate(labels)}\n",
    "    w2i = {w:i for i,w in enumerate(words)}\n",
    "    return l2i, w2i, labels, words\n",
    "\n",
    "train = read_dataset(\"data/trees/train.txt\")\n",
    "dev = read_dataset(\"data/trees/dev.txt\")\n",
    "\n",
    "l2i, w2i, i2l, i2w = get_vocabs(train)\n",
    "ntags = len(l2i)\n",
    "nwords = len(w2i)\n",
    "\n",
    "# Start DyNet and define trainer\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Socher-style Tree RNN\n",
    "class TreeRNNBuilder(object):\n",
    "    def __init__(self, model, word_vocab, hdim):\n",
    "        self.W = model.add_parameters((hdim, 2*hdim))\n",
    "        self.E = model.add_lookup_parameters((len(word_vocab),hdim))\n",
    "        self.w2i = word_vocab\n",
    "\n",
    "    def expr_for_tree(self, tree):\n",
    "        if tree.isleaf():\n",
    "            return self.E[self.w2i.get(tree.label,0)]\n",
    "        if len(tree.children) == 1:\n",
    "            assert(tree.children[0].isleaf())\n",
    "            expr = self.expr_for_tree(tree.children[0])\n",
    "            return expr\n",
    "        assert(len(tree.children) == 2),tree.children[0]\n",
    "        e1 = self.expr_for_tree(tree.children[0])\n",
    "        e2 = self.expr_for_tree(tree.children[1])\n",
    "        W = dy.parameter(self.W)\n",
    "        expr = dy.tanh(W*dy.concatenate([e1,e2]))\n",
    "        return expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tai-style Tree LSTM\n",
    "class TreeLSTMBuilder(object):\n",
    "    def __init__(self, model, word_vocab, wdim, hdim):\n",
    "        self.WS = [model.add_parameters((hdim, wdim)) for _ in \"iou\"]\n",
    "        self.US = [model.add_parameters((hdim, 2*hdim)) for _ in \"iou\"]\n",
    "        self.UFS =[model.add_parameters((hdim, hdim)) for _ in \"ff\"]\n",
    "        self.BS = [model.add_parameters(hdim) for _ in \"iouf\"]\n",
    "        self.E = model.add_lookup_parameters((len(word_vocab),wdim))\n",
    "        self.w2i = word_vocab\n",
    "\n",
    "    def expr_for_tree(self, tree):\n",
    "        if tree.isleaf():\n",
    "            return self.E[self.w2i.get(tree.label,0)]\n",
    "        if len(tree.children) == 1:\n",
    "            assert(tree.children[0].isleaf())\n",
    "            emb = self.expr_for_tree(tree.children[0])\n",
    "            Wi,Wo,Wu   = [dy.parameter(w) for w in self.WS]\n",
    "            bi,bo,bu,_ = [dy.parameter(b) for b in self.BS]\n",
    "            i = dy.logistic(Wi*emb + bi)\n",
    "            o = dy.logistic(Wo*emb + bo)\n",
    "            u = dy.tanh(    Wu*emb + bu)\n",
    "            c = dy.cmult(i,u)\n",
    "            expr = dy.cmult(o,dy.tanh(c))\n",
    "            return expr\n",
    "        assert(len(tree.children) == 2),tree.children[0]\n",
    "        e1 = self.expr_for_tree(tree.children[0])\n",
    "        e2 = self.expr_for_tree(tree.children[1])\n",
    "        Ui,Uo,Uu = [dy.parameter(u) for u in self.US]\n",
    "        Uf1,Uf2 = [dy.parameter(u) for u in self.UFS]\n",
    "        bi,bo,bu,bf = [dy.parameter(b) for b in self.BS]\n",
    "        e = dy.concatenate([e1,e2])\n",
    "        i = dy.logistic(Ui*e + bi)\n",
    "        o = dy.logistic(Uo*e + bo)\n",
    "        f1 = dy.logistic(Uf1*e1 + bf)\n",
    "        f2 = dy.logistic(Uf2*e2 + bf)\n",
    "        u = dy.tanh(    Uu*e + bu)\n",
    "        c = dy.cmult(i,u) + dy.cmult(f1,e1) + dy.cmult(f2,e2)\n",
    "        h = dy.cmult(o,dy.tanh(c))\n",
    "        expr = h\n",
    "        return expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "EMB_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "builder = TreeRNNBuilder(model, w2i, HID_SIZE)\n",
    "# builder = TreeLSTMBuilder(model, w2i, HID_SIZE, EMB_SIZE)\n",
    "W_sm = model.add_parameters((ntags, HID_SIZE))        # Softmax weights\n",
    "b_sm = model.add_parameters((ntags))                  # Softmax bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to calculate scores for one value\n",
    "def calc_scores(tree):\n",
    "  dy.renew_cg()\n",
    "  emb = builder.expr_for_tree(tree)\n",
    "  W_sm_exp = dy.parameter(W_sm)\n",
    "  b_sm_exp = dy.parameter(b_sm)\n",
    "  return W_sm_exp * emb + b_sm_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=1.6307, time=3.08s\n",
      "iter 0: test acc=0.2616\n",
      "iter 1: train loss/sent=1.5502, time=3.21s\n",
      "iter 1: test acc=0.3061\n",
      "iter 2: train loss/sent=1.4464, time=3.12s\n",
      "iter 2: test acc=0.3179\n",
      "iter 3: train loss/sent=1.3092, time=3.28s\n",
      "iter 3: test acc=0.3261\n",
      "iter 4: train loss/sent=1.1444, time=3.48s\n",
      "iter 4: test acc=0.3052\n",
      "iter 5: train loss/sent=0.9875, time=3.60s\n",
      "iter 5: test acc=0.3170\n",
      "iter 6: train loss/sent=0.8731, time=3.39s\n",
      "iter 6: test acc=0.3179\n",
      "iter 7: train loss/sent=0.7864, time=3.25s\n",
      "iter 7: test acc=0.3197\n"
     ]
    }
   ],
   "source": [
    "for ITER in range(100):\n",
    "  # Perform training\n",
    "  random.shuffle(train)\n",
    "  train_loss = 0.0\n",
    "  start = time.time()\n",
    "  for tree in train:\n",
    "    my_loss = dy.pickneglogsoftmax(calc_scores(tree), l2i[tree.label])\n",
    "    train_loss += my_loss.value()\n",
    "    my_loss.backward()\n",
    "    trainer.update()\n",
    "  print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (ITER, train_loss/len(train), time.time()-start))\n",
    "  # Perform testing\n",
    "  test_correct = 0.0\n",
    "  for tree in dev:\n",
    "    scores = calc_scores(tree).npvalue()\n",
    "    predict = np.argmax(scores)\n",
    "    if predict == l2i[tree.label]:\n",
    "      test_correct += 1\n",
    "  print(\"iter %r: test acc=%.4f\" % (ITER, test_correct/len(dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:benchmark2]",
   "language": "python",
   "name": "conda-env-benchmark2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
